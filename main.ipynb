{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing all the packages you need.\n",
    "\n",
    "import os\n",
    "\n",
    "packages = (\"numpy\", \"pandas\", \"matplotlib\", \"urllib\", \"tensorflow==1.6\", \"scikit-learn\")\n",
    "\n",
    "stream = os.popen(\"pip install \" + \"requests\")\n",
    "output = stream.read()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import numpy as np\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from the file. For the file please refer to\n",
    "# https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs/data#\n",
    "\n",
    "df = pd.read_csv('hpq.us.txt', delimiter=',', usecols=['Date','Open','High','Low','Close'])\n",
    "\n",
    "print('Data Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the data\n",
    "#1. Sorting the data by date\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "#2. Adding the mid-value for each entry\n",
    "df[\"mid\"] = (df['Low'] + df['High'])/2.0\n",
    "\n",
    "#3. Number ofe entries\n",
    "print(\"The dataset has {} entries\".format(df.shape[0]))\n",
    "\n",
    "#3. Printing the first 10 rows to check\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Data Visualtization\n",
    "#1. Creating a figure with a specific size\n",
    "plt.figure(figsize = (20,10))\n",
    "\n",
    "#2. Plotting the data-points of mid-prices\n",
    "plt.plot(range(df.shape[0]),df['mid'])\n",
    "\n",
    "#3. Creating x-axis point names (dates) on the axis\n",
    "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\n",
    "\n",
    "#4. Creating a label for the x axis\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "\n",
    "#5. Creating a label for the y axis\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "\n",
    "#6. Showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data into train-test sets\n",
    "train_data = df['mid'][:11000]\n",
    "test_data = df['mid'][11000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data so that the points lie within (0,1) range (windowed-normalization)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)\n",
    "\n",
    "smoothing_window_size = 2500\n",
    "\n",
    "for di in range(0,10000,smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "    \n",
    "# Normalizing the last bit of remaining data\n",
    "\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])\n",
    "\n",
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform exponential moving average smoothing\n",
    "# So the data will have a smoother curve than the original ragged data\n",
    "\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(11000):\n",
    "  EMA = gamma*train_data[ti] + (1-gamma)*EMA\n",
    "  train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Step Ahead Prediction\n",
    "# Predicting the value at time t based on previous (windo-sized) number of points (Simple Averaging)\n",
    "\n",
    "window_size = 100\n",
    "N = train_data.size\n",
    "std_avg_predictions = []\n",
    "std_avg_x = []\n",
    "mse_errors = []\n",
    "\n",
    "for pred_idx in range(window_size,N):\n",
    "\n",
    "    if pred_idx >= N:\n",
    "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
    "    else:\n",
    "        date = df.loc[pred_idx,'Date']\n",
    "    \n",
    "    # Calculating the mean and using it as the predicted value\n",
    "    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n",
    "    \n",
    "    # Calculating the mean squared error based on the difference between\n",
    "    # the actual value and the predicted one\n",
    "    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    std_avg_x.append(date)\n",
    "\n",
    "print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the result from simple average prediction\n",
    "\n",
    "#1. Creating a figure with a specific size\n",
    "plt.figure(figsize = (18,9))\n",
    "\n",
    "#2. Plotting the mid-prices from the observed values\n",
    "plt.plot(range(df.shape[0]), all_mid_data, color='b', label='Observed Values')\n",
    "\n",
    "#3. Plotting the result from prediction\n",
    "plt.plot(range(window_size,N),std_avg_predictions, color='r', label='Predicted Values')\n",
    "\n",
    "#4. Creating a label for the x axis\n",
    "plt.xlabel('Date')\n",
    "\n",
    "#5. Creating a label for the y axis\n",
    "plt.ylabel('Mid Price')\n",
    "\n",
    "#6. Creating a legend at the corner of the figure\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "#7. Showing the plot on the screen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential Moving Average Prediction\n",
    "\n",
    "window_size = 100\n",
    "N = train_data.size\n",
    "\n",
    "run_avg_predictions = []\n",
    "# run_avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "running_mean = 0.0\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred_idx in range(1,N):\n",
    "\n",
    "    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n",
    "    \n",
    "    # Calculating the moving average and using it as a prediction\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    \n",
    "    # Calculating the mean squared error based on the difference between\n",
    "    # the actual value and the predicted one\n",
    "    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "#     run_avg_x.append(date)\n",
    "\n",
    "print('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the result from moving average prediction\n",
    "\n",
    "#1. Creating a figure with a specific size\n",
    "plt.figure(figsize = (18,9))\n",
    "\n",
    "#2. Plotting the mid-prices from the observed values\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='Observed Values')\n",
    "\n",
    "#3. Plotting the result from prediction\n",
    "plt.plot(range(0,N),run_avg_predictions,color='red', label='Predicted Values')\n",
    "\n",
    "#4. Creating a label for the x axis\n",
    "plt.xlabel('Date')\n",
    "\n",
    "#5. Creating a label for the y axis\n",
    "plt.ylabel('Mid Price')\n",
    "\n",
    "#6. Creating a legend at the corner of the figure\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "#7. Showing the plot on the screen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class of data generator with data augmentation\n",
    "# along with unrolling and batching (tunable in the __init__ magic function) \n",
    "\n",
    "class DataGeneratorSeq(object):\n",
    "\n",
    "    def __init__(self,prices,batch_size,num_unroll):\n",
    "        self._prices = prices\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length //self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b]+1>=self._prices_length:\n",
    "                #self._cursor[b] = b * self._segments\n",
    "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        init_data, init_label = None,None\n",
    "        for ui in range(self._num_unroll):\n",
    "\n",
    "            data, labels = self.next_batch()    \n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n",
    "\n",
    "\n",
    "\n",
    "dg = DataGeneratorSeq(train_data,5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    print('\\tInputs: ',dat )\n",
    "    print('\\n\\tOutput:',lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the hyperparameters of the model\n",
    "\n",
    "# Data dimensionality which is 1 in our case.\n",
    "D = 1\n",
    "\n",
    "# Number of time steps to look into the future (unrolling)\n",
    "num_unrollings = 50\n",
    "\n",
    "# Number of samples in a batch\n",
    "batch_size = 500\n",
    "\n",
    "# Number of hidden nodes in each layer of the deep LSTM network\n",
    "num_nodes = [200,200,150]\n",
    "\n",
    "# Number of network layers\n",
    "n_layers = len(num_nodes)\n",
    "\n",
    "# Dropout prob.\n",
    "dropout = 0.2 # dropout amount\n",
    "\n",
    "# This is important in case you run this multiple times\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating LSTM cells with corresponding number of nodes and layers\n",
    "\n",
    "lstm_cells = [\n",
    "    tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n",
    "                            state_is_tuple=True,\n",
    "                            initializer= tf.contrib.layers.xavier_initializer()\n",
    "                           )\n",
    " for li in range(n_layers)]\n",
    "\n",
    "# Implementing the dropout\n",
    "drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n",
    "    lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout\n",
    ") for lstm in lstm_cells]\n",
    "\n",
    "\n",
    "# Creating a multi-cell RNN\n",
    "drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "w = tf.get_variable('w',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b',initializer=tf.random_uniform([1],-0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cell state and hidden state variables to maintain the state of the LSTM\n",
    "\n",
    "c, h = [],[]\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "    c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "    h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "    initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n",
    "\n",
    "# Do several tensor transofmations, because the function dynamic_rnn requires the output to be of\n",
    "# a specific format. Read more at: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)\n",
    "\n",
    "# all_outputs is [seq_length, batch_size, num_nodes]\n",
    "all_lstm_outputs, state = tf.nn.dynamic_rnn(\n",
    "    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),\n",
    "    time_major = True, dtype=tf.float32)\n",
    "\n",
    "all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n",
    "\n",
    "all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n",
    "\n",
    "split_outputs = tf.split(all_outputs,num_unrollings,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When calculating the loss you need to be careful about the exact form, because you calculate\n",
    "# loss of all the unrolled steps at the same time\n",
    "#The mean error of each batch is calculated and then the sum of that over all unrolled steps are calculated.\n",
    "\n",
    "print('Defining training Loss')\n",
    "loss = 0.0\n",
    "with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+\n",
    "                             [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n",
    "    for ui in range(num_unrollings):\n",
    "        loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n",
    "\n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n",
    "    tf_min_learning_rate)\n",
    "\n",
    "# Optimization process\n",
    "\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimizer = optimizer.apply_gradients(zip(gradients, v))\n",
    "\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the TF functions that are related to the prediction\n",
    "\n",
    "print('Defining prediction related TF functions')\n",
    "\n",
    "sample_inputs = tf.placeholder(tf.float32, shape=[1,D])\n",
    "\n",
    "# Maintaining LSTM state for prediction stage\n",
    "sample_c, sample_h, initial_sample_state = [],[],[]\n",
    "\n",
    "for li in range(n_layers):\n",
    "    sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "    sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "    initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n",
    "\n",
    "reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n",
    "                               *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n",
    "\n",
    "sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n",
    "                                   initial_state=tuple(initial_sample_state),\n",
    "                                   time_major = True,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "with tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n",
    "                              [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n",
    "    sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the LSTM network (training, validation, testing)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# Interval you make test predictions\n",
    "valid_summary = 1\n",
    "\n",
    "# Number of steps you continously predict for\n",
    "n_predict_once = 50\n",
    "\n",
    "# Full length of the training data\n",
    "train_seq_length = train_data.size\n",
    "\n",
    "# List to hold all train losses\n",
    "train_mse_ot = []\n",
    "\n",
    "# List to hold all test losses\n",
    "test_mse_ot = []\n",
    "\n",
    "# List to hold all predictions\n",
    "predictions_over_time = []\n",
    "\n",
    "# Setting up the tensorflow session\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Initializing variables\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Learning rate decay setup\n",
    "loss_nondecrease_count = 0\n",
    "\n",
    "# Decrease the learning rate after the test loss has remained the same for this many steps\n",
    "loss_nondecrease_threshold = 2 \n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# Defining the data generator\n",
    "data_gen = DataGeneratorSeq(train_data,batch_size,num_unrollings)\n",
    "\n",
    "x_axis_seq = []\n",
    "\n",
    "# Points you start your test predictions from\n",
    "test_points_seq = np.arange(11000,12000,50).tolist()\n",
    "\n",
    "for ep in range(epochs):       \n",
    "\n",
    "    # ========================= Training =====================================\n",
    "    for step in range(train_seq_length//batch_size):\n",
    "\n",
    "        u_data, u_labels = data_gen.unroll_batches()\n",
    "\n",
    "        feed_dict = {}\n",
    "        for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "            feed_dict[train_inputs[ui]] = dat.reshape(-1,1)\n",
    "            feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)\n",
    "\n",
    "        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})\n",
    "\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "        average_loss += l\n",
    "\n",
    "    # ============================ Validation ==============================\n",
    "    if (ep+1) % valid_summary == 0:\n",
    "\n",
    "        average_loss = average_loss/(valid_summary*(train_seq_length//batch_size))\n",
    "\n",
    "        # The average loss\n",
    "        if (ep+1)%valid_summary==0:\n",
    "            print('Average loss at step %d: %f' % (ep+1, average_loss))\n",
    "\n",
    "        train_mse_ot.append(average_loss)\n",
    "        \n",
    "        # reset loss\n",
    "        average_loss = 0 \n",
    "\n",
    "        predictions_seq = []\n",
    "\n",
    "        mse_test_loss_seq = []\n",
    "\n",
    "      # ===================== Updating State and Making Predicitons ========================\n",
    "    for w_i in test_points_seq:\n",
    "        mse_test_loss = 0.0\n",
    "        our_predictions = []\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          # Only calculate x_axis values in the first validation epoch\n",
    "          x_axis=[]\n",
    "\n",
    "        # Feed in the recent past behavior of stock prices\n",
    "        # to make predictions from that point onwards\n",
    "        for tr_i in range(w_i-num_unrollings+1,w_i-1):\n",
    "            current_price = all_mid_data[tr_i]\n",
    "            feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)    \n",
    "            _ = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "        feed_dict = {}\n",
    "\n",
    "        current_price = all_mid_data[w_i-1]\n",
    "\n",
    "        feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n",
    "\n",
    "        # Make predictions for this many steps\n",
    "        # Each prediction uses previous prediciton as it's current input\n",
    "        for pred_i in range(n_predict_once):\n",
    "\n",
    "            pred = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "            our_predictions.append(np.asscalar(pred))\n",
    "\n",
    "            feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n",
    "\n",
    "            if (ep+1)-valid_summary==0:\n",
    "                # Only calculate x_axis values in the first validation epoch\n",
    "                x_axis.append(w_i+pred_i)\n",
    "\n",
    "            mse_test_loss += 0.5*(pred-all_mid_data[w_i+pred_i])**2\n",
    "\n",
    "        session.run(reset_sample_states)\n",
    "\n",
    "        predictions_seq.append(np.array(our_predictions))\n",
    "\n",
    "        mse_test_loss /= n_predict_once\n",
    "        mse_test_loss_seq.append(mse_test_loss)\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "            x_axis_seq.append(x_axis)\n",
    "\n",
    "        current_test_mse = np.mean(mse_test_loss_seq)\n",
    "\n",
    "      # Learning rate decay logic\n",
    "        if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n",
    "            loss_nondecrease_count += 1\n",
    "        else:\n",
    "            loss_nondecrease_count = 0\n",
    "\n",
    "        if loss_nondecrease_count > loss_nondecrease_threshold :\n",
    "            session.run(inc_gstep)\n",
    "            loss_nondecrease_count = 0\n",
    "            print('\\tDecreasing learning rate by 0.5')\n",
    "\n",
    "        test_mse_ot.append(current_test_mse)\n",
    "        print('\\tTest MSE: %.5f'%np.mean(mse_test_loss_seq))\n",
    "        predictions_over_time.append(predictions_seq)\n",
    "        print('\\tFinished Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulisation of the predictions\n",
    "\n",
    "# Change the number below to the epoc you got the best results from\n",
    "best_prediction_epoch = 28\n",
    "\n",
    "plt.figure(figsize = (18,18))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
    "\n",
    "# Plotting how the predictions change over time\n",
    "# Plot older predictions with low alpha and newer predictions with high alpha\n",
    "start_alpha = 0.25\n",
    "alpha  = np.arange(start_alpha,1.1,(1.0-start_alpha)/len(predictions_over_time[::3]))\n",
    "for p_i,p in enumerate(predictions_over_time[::3]):\n",
    "    for xval,yval in zip(x_axis_seq,p):\n",
    "        plt.plot(xval,yval,color='r',alpha=alpha[p_i])\n",
    "\n",
    "plt.title('Test Predictions Over Time',fontsize=18)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.xlim(11000,12500)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "# Predicting the best test prediction you got\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
    "for xval,yval in zip(x_axis_seq,predictions_over_time[best_prediction_epoch]):\n",
    "    plt.plot(xval,yval,color='r')\n",
    "\n",
    "plt.title('Best Test Predictions Over Time',fontsize=18)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.xlim(11000,12500)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
